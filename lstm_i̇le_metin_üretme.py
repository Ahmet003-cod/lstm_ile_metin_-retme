# -*- coding: utf-8 -*-
"""LSTM_İle_Metin_üretme.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yrEy5dyHg5qRdSSDDFUOjxw0jXgmKvPu
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding,LSTM,Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

data="/content/gunluk_cumleler.txt"

path = "/content/gunluk_cumleler.txt"
with open(path, "r", encoding="utf-8") as f:
    data = f.readlines() # Dosyadaki her satırı bir liste elemanı yapar

# 2. Preprocessing
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data)

# 3. Değişkeni tanımla (S takısına dikkat!)
total_words = len(tokenizer.word_index) + 1

# 4. Print ederken aynı ismi kullan
print(f"total_words: {total_words}")

# n-gram dizileri oluştur yani her cümleden kisa diziler oluştur(embedding)
# 3-gram :kelimelri indexlere(sayilara) cevir ,["kelimeleri (sayilara) çevir,["kelimeleri indexlere çevşr"]"]
input_squences=[]
for text in data:
    token_list=tokenizer.texts_to_sequences([text])[0]
    for i in range(1,len(token_list)):
      n_gram_sequence=token_list[:i+1]
      input_squences.append(n_gram_sequence)

print(f" input squencees\n{input_squences}")

#padding:farki uzunluktaki dizileri sabitle
max_sequences_length=max([len(x) for x in input_squences])
input_sequences=pad_sequences(input_squences,maxlen=max_sequences_length,padding="pre")

print(f"after padding:\n{input_sequences}")

# girdi (x) ve hedef değişkenler (y) ayir
X=input_sequences[:,:-1]#n-1  kelimeyi giriş olarak seç
y=input_sequences[:,-1] # n inci kelimeyi tahmin et

#hedef değişken one hot encoding
y=tf.keras.utils.to_categorical(y,num_classes=total_words)
print(f"hedef değişkenler:\n{y}")

#lstm modeli tanimla
model=Sequential()
model.add(Embedding(total_words,50,input_length=max_sequences_length-1))#embedding katmanı
model.add(LSTM(100))
model.add(Dense(total_words,activation="softmax"))
"""
X=[bugün hava açık]
y=[güzel]
"""

#compile
model.compile(loss="categorical_crossentropy",optimizer="adam",metrics=["accuracy"])
print(model.summary())

#eğitimi başlat
model.fit(X,y,epochs=100,verbose=1)

#örnek üretim testi (metin üretimi)
def generate_text(seed_text,next_words):
  for _ in range(next_words):
    token_list=tokenizer.texts_to_sequences([seed_text])[0]#tokinizer
    token_list=pad_sequences([token_list],maxlen=max_sequences_length-1,padding="pre")#padding
    predicted_probs=model.predict(token_list,verbose=0)#tahmin
    predicted_index=np.argmax(predicted_probs,axis=-1)[0]#
    predict_word=tokenizer.index_word[predicted_index]#indexi kelimeye çevir
    seed_text+=" "+predict_word
  return seed_text

"""
seed_text=bu sabah
predicted_word=okula
seed_text=bu sabah okula
"""
print(generate_text("bugün ",5))

